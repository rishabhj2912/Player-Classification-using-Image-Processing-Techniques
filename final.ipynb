{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "from scipy.spatial.distance import cosine\n",
    "import torch.nn as nn\n",
    "from colorthief import ColorThief\n",
    "import clip\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction the Players using YOLOv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = 'yolov3.weights'\n",
    "config_path = 'yolov3.cfg'\n",
    "class_names_path = 'coco.names'\n",
    "net = cv2.dnn.readNet(weights_path, config_path)\n",
    "with open(class_names_path, 'r') as f:\n",
    "    class_names = f.read().strip().split('\\n')\n",
    "image_path = 'output.jpg'\n",
    "image = cv2.imread(image_path)\n",
    "height, width = image.shape[:2]\n",
    "blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "net.setInput(blob)\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = net.forward(output_layers)\n",
    "boxes = []\n",
    "confidences = []\n",
    "class_ids = []\n",
    "\n",
    "for detection in detections:\n",
    "    for object_detection in detection:\n",
    "        scores = object_detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.6 and class_names[class_id] == 'person':\n",
    "            center_x = int(object_detection[0] * width)\n",
    "            center_y = int(object_detection[1] * height)\n",
    "            w = int(object_detection[2] * width)\n",
    "            h = int(object_detection[3] * height)\n",
    "            x = int(center_x - w / 2)\n",
    "            y = int(center_y - h / 2)\n",
    "            boxes.append([x, y, w, h])\n",
    "            confidences.append(float(confidence))\n",
    "            class_ids.append(class_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making bounding boxes around the detected humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "if len(indices) > 0:\n",
    "    for i in indices.flatten():\n",
    "        box = boxes[i]\n",
    "        x, y, w, h = box[0], box[1], box[2], box[3]\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "        cv2.putText(image, 'Person', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'detected_players.jpg'\n",
    "cv2.imwrite(output_path, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'detected_players'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i, index in enumerate(indices.flatten()):\n",
    "    box = boxes[index]\n",
    "    x, y, w, h = box[0], box[1], box[2], box[3]\n",
    "    \n",
    "    cropped_image = image[y:y+h, x:x+w]\n",
    "    \n",
    "    player_image_path = os.path.join(output_dir, f'player_{i+1}.jpg')\n",
    "    cv2.imwrite(player_image_path, cropped_image)\n",
    "    print(f\"Saved cropped image to {player_image_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1: Using feature vectors of the players and finding Cosine Similarity to classify the players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Inception to extract the feature vectors of the players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "inception = models.inception_v3(pretrained=True)\n",
    "inception.fc = nn.Identity()\n",
    "inception.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_vector(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    img_tensor = preprocess(image).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        feature_vector = inception(img_tensor).numpy().flatten()\n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "image_paths = ['result_detected_players/player_1.jpg' ,'result_detected_players/player_2.jpg','result_detected_players/player_3.jpg','result_detected_players/player_4.jpg','result_detected_players/player_5.jpg']\n",
    "feature_vectors = [generate_feature_vector(image_path) for image_path in image_paths]\n",
    "\n",
    "source_dir = 'top1'\n",
    "files = os.listdir(source_dir)\n",
    "comp_img='two_players_bot/24082_86_142_128.jpg'\n",
    "feature_vectors_compare = [generate_feature_vector(comp_img)]\n",
    "\n",
    "feature_vectors_compare = np.array(feature_vectors_compare)\n",
    "\n",
    "average_feature_vector = np.median(feature_vectors_compare, axis=0)\n",
    "\n",
    "similarities = []\n",
    "for image_path, person_vector in zip(image_paths, feature_vectors):\n",
    "    similarity = 1 - cosine(average_feature_vector, person_vector)\n",
    "    similarities.append((image_path, similarity))\n",
    "print(similarities)\n",
    "most_similar_image_path, highest_similarity = max(similarities, key=lambda x: x[1])\n",
    "\n",
    "print(f\"The most similar person is represented by the image {most_similar_image_path} with a similarity score of {highest_similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification of the players from bottom into player1 and player2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(299),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "inception = models.inception_v3(pretrained=True, transform_input=False)\n",
    "inception.eval()\n",
    "\n",
    "def generate_feature_vector(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    img_tensor = preprocess(image).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        feature_vector = inception(img_tensor).numpy().flatten()\n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "image_paths = [ 'detected_players/player_1.jpg', 'detected_players/player_2.jpg']\n",
    "source_dir = 'two_players_bot'\n",
    "comparison_image_paths = [os.path.join(source_dir, file) for file in os.listdir(source_dir)]\n",
    "\n",
    "feature_vectors = [generate_feature_vector(image_path) for image_path in image_paths]\n",
    "\n",
    "output_dirs = [f'output_folder/player_{i+1}' for i in range(len(image_paths))]\n",
    "for output_dir in output_dirs:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "comparison_vectors = {img_path: generate_feature_vector(img_path) for img_path in comparison_image_paths}\n",
    "\n",
    "for comp_path, comp_vector in comparison_vectors.items():\n",
    "    similarities = []\n",
    "    for i, person_vector in enumerate(feature_vectors):\n",
    "        similarity = 1 - cosine(person_vector, comp_vector)\n",
    "        similarities.append((i, similarity))\n",
    "    \n",
    "    most_similar_index, highest_similarity = max(similarities, key=lambda x: x[1])\n",
    "    \n",
    "    output_path = os.path.join(output_dirs[most_similar_index], os.path.basename(comp_path))\n",
    "    shutil.copy(comp_path, output_path)\n",
    "\n",
    "    print(f\"Image {comp_path} is most similar to player_{most_similar_index + 1} with a similarity score of {highest_similarity}\")\n",
    "\n",
    "for comp_path, comp_vector in comparison_vectors.items():\n",
    "    print(f\"Similarity scores for {comp_path}:\")\n",
    "    for i, person_vector in enumerate(feature_vectors):\n",
    "        similarity = 1 - cosine(person_vector, comp_vector)\n",
    "        print(f\"\\tCompared to player_{i + 1}: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2: Using Clip (Vision Transformers) to find the Image Similarity to classify the players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Clip to form textual embeddings of the players and then finding the similarity between the players to classify them in player1 and player2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "def calculate_similarity(image_path, ref_features):\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    image_features = model.encode_image(image)\n",
    "    similarity = cos(image_features[0], ref_features[0]).item()\n",
    "    return (similarity + 1) / 2 \n",
    "\n",
    "image1_path = \"detected_players/player_1.jpg\"\n",
    "image2_path = \"detected_players/player_2.jpg\"\n",
    "\n",
    "image1_preprocess = preprocess(Image.open(image1_path)).unsqueeze(0).to(device)\n",
    "image1_features = model.encode_image(image1_preprocess)\n",
    "\n",
    "image2_preprocess = preprocess(Image.open(image2_path)).unsqueeze(0).to(device)\n",
    "image2_features = model.encode_image(image2_preprocess)\n",
    "\n",
    "input_folder_path = \"two_players_bot\"\n",
    "output_folder1 = \"player1_clip\"\n",
    "output_folder2 = \"player2_clip\"\n",
    "\n",
    "os.makedirs(output_folder1, exist_ok=True)\n",
    "os.makedirs(output_folder2, exist_ok=True)\n",
    "\n",
    "for image_name in os.listdir(input_folder_path):\n",
    "    image_path = os.path.join(input_folder_path, image_name)\n",
    "    \n",
    "    if os.path.isfile(image_path): \n",
    "        sim1 = calculate_similarity(image_path, image1_features)\n",
    "        sim2 = calculate_similarity(image_path, image2_features)\n",
    "        \n",
    "        if sim1 > sim2:\n",
    "            classification = \"Image 1\"\n",
    "            shutil.copy(image_path, os.path.join(output_folder1, image_name))\n",
    "        else:\n",
    "            classification = \"Image 2\"\n",
    "            shutil.copy(image_path, os.path.join(output_folder2, image_name))\n",
    "        \n",
    "        print(f\"Image: {image_name} | Similarity to Image 1: {sim1:.4f} | Similarity to Image 2: {sim2:.4f} | Classified as: {classification}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Clip to form textual embeddings of the players and then finding the similarity between the players to classify them in player3 and player4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "def calculate_similarity(image_path, ref_features):\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    image_features = model.encode_image(image)\n",
    "    similarity = cos(image_features[0], ref_features[0]).item()\n",
    "    return (similarity + 1) / 2  \n",
    "\n",
    "image1_path = \"detected_players/player_3.jpg\"\n",
    "image2_path = \"detected_players/player_4.jpg\"\n",
    "\n",
    "image1_preprocess = preprocess(Image.open(image1_path)).unsqueeze(0).to(device)\n",
    "image1_features = model.encode_image(image1_preprocess)\n",
    "\n",
    "image2_preprocess = preprocess(Image.open(image2_path)).unsqueeze(0).to(device)\n",
    "image2_features = model.encode_image(image2_preprocess)\n",
    "\n",
    "input_folder_path = \"two_players_top\"\n",
    "output_folder1 = \"player3_clip\"\n",
    "output_folder2 = \"player4_clip\"\n",
    "\n",
    "os.makedirs(output_folder1, exist_ok=True)\n",
    "os.makedirs(output_folder2, exist_ok=True)\n",
    "\n",
    "for image_name in os.listdir(input_folder_path):\n",
    "    image_path = os.path.join(input_folder_path, image_name)\n",
    "    \n",
    "    if os.path.isfile(image_path):\n",
    "        sim1 = calculate_similarity(image_path, image1_features)\n",
    "        sim2 = calculate_similarity(image_path, image2_features)\n",
    "        \n",
    "        if sim1 > sim2:\n",
    "            classification = \"Image 1\"\n",
    "            shutil.copy(image_path, os.path.join(output_folder1, image_name))\n",
    "        else:\n",
    "            classification = \"Image 2\"\n",
    "            shutil.copy(image_path, os.path.join(output_folder2, image_name))\n",
    "        \n",
    "        print(f\"Image: {image_name} | Similarity to Image 1: {sim1:.4f} | Similarity to Image 2: {sim2:.4f} | Classified as: {classification}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 3: Using Colour Layout descriptor with custom weights to classify the players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diving the image into a grid and then finding the colour layout descriptor of the grid and giving more weightage to the centre of the image to find the players and classify them in player1 and player2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def custom_weights(grid_size=(128, 128), center_fraction=0.4, center_weight=2.0):\n",
    "    \"\"\"Generate a custom weight matrix with higher weights in the center.\"\"\"\n",
    "    weights = np.ones(grid_size)\n",
    "    center_start = int((1 - center_fraction) * grid_size[0] / 2)\n",
    "    center_end = grid_size[0] - center_start\n",
    "\n",
    "    for i in range(center_start, center_end):\n",
    "        for j in range(center_start, center_end):\n",
    "            weights[i, j] = center_weight\n",
    "\n",
    "    return weights\n",
    "\n",
    "def color_layout_descriptor(image, grid_size=(128, 128), weights=None):\n",
    "    resized_image = cv2.resize(image, (256, 256))\n",
    "    \n",
    "    ycrcb_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2YCrCb)\n",
    "    \n",
    "    height, width, _ = ycrcb_image.shape\n",
    "    grid_h, grid_w = height // grid_size[0], width // grid_size[1]\n",
    "    \n",
    "    descriptor = []\n",
    "    for i in range(0, height, grid_h):\n",
    "        for j in range(0, width, grid_w):\n",
    "            cell = ycrcb_image[i:i+grid_h, j:j+grid_w]\n",
    "            mean_color = np.mean(cell.reshape(-1, 3), axis=0)\n",
    "            descriptor.append(mean_color)\n",
    "    \n",
    "    descriptor = np.array(descriptor)\n",
    "    \n",
    "    if weights is not None:\n",
    "        descriptor *= weights.reshape(-1, 1)\n",
    "    \n",
    "    return descriptor.flatten()\n",
    "\n",
    "def compare_descriptors(desc1, desc2):\n",
    "    return euclidean_distances([desc1], [desc2])[0][0]\n",
    "\n",
    "def classify_images(reference1, reference2, folder, output_folder1, output_folder2):\n",
    "    grid_size = (128, 128)\n",
    "    weights = custom_weights(grid_size, center_fraction=0.4, center_weight=2.0)\n",
    "    \n",
    "    ref1_descriptor = color_layout_descriptor(reference1, grid_size, weights)\n",
    "    ref2_descriptor = color_layout_descriptor(reference2, grid_size, weights)\n",
    "    \n",
    "    os.makedirs(output_folder1, exist_ok=True)\n",
    "    os.makedirs(output_folder2, exist_ok=True)\n",
    "    \n",
    "    for filename in os.listdir(folder):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        \n",
    "        if os.path.isfile(filepath):\n",
    "            image = cv2.imread(filepath)\n",
    "            image_descriptor = color_layout_descriptor(image, grid_size, weights)\n",
    "            \n",
    "            similarity_to_ref1 = compare_descriptors(ref1_descriptor, image_descriptor)\n",
    "            similarity_to_ref2 = compare_descriptors(ref2_descriptor, image_descriptor)\n",
    "            \n",
    "            if similarity_to_ref1 < similarity_to_ref2:\n",
    "                shutil.copy(filepath, os.path.join(output_folder1, filename))\n",
    "            else:\n",
    "                shutil.copy(filepath, os.path.join(output_folder2, filename))\n",
    "\n",
    "\n",
    "reference_image1 = cv2.imread('segmented_image1.jpg')\n",
    "reference_image2 = cv2.imread('segmented_image2.jpg')\n",
    "\n",
    "input_folder = 'two_players_bot'\n",
    "\n",
    "output_folder1 = 'player1_colour'\n",
    "output_folder2 = 'player2_colour'\n",
    "\n",
    "classify_images(reference_image1, reference_image2, input_folder, output_folder1, output_folder2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diving the image into a grid and then finding the colour layout descriptor of the grid and giving more weightage to the centre of the image to find the players and classify them in player3 and player4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def custom_weights(grid_size=(128, 128), center_fraction=0.4, center_weight=2.0):\n",
    "    \"\"\"Generate a custom weight matrix with higher weights in the center.\"\"\"\n",
    "    weights = np.ones(grid_size)\n",
    "    center_start = int((1 - center_fraction) * grid_size[0] / 2)\n",
    "    center_end = grid_size[0] - center_start\n",
    "\n",
    "    for i in range(center_start, center_end):\n",
    "        for j in range(center_start, center_end):\n",
    "            weights[i, j] = center_weight\n",
    "\n",
    "    return weights\n",
    "\n",
    "def color_layout_descriptor(image, grid_size=(128, 128), weights=None):\n",
    "    resized_image = cv2.resize(image, (256, 256))\n",
    "    \n",
    "    ycrcb_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2YCrCb)\n",
    "    \n",
    "    height, width, _ = ycrcb_image.shape\n",
    "    grid_h, grid_w = height // grid_size[0], width // grid_size[1]\n",
    "    \n",
    "    descriptor = []\n",
    "    for i in range(0, height, grid_h):\n",
    "        for j in range(0, width, grid_w):\n",
    "            cell = ycrcb_image[i:i+grid_h, j:j+grid_w]\n",
    "            mean_color = np.mean(cell.reshape(-1, 3), axis=0)\n",
    "            descriptor.append(mean_color)\n",
    "    \n",
    "    descriptor = np.array(descriptor)\n",
    "    \n",
    "    if weights is not None:\n",
    "        descriptor *= weights.reshape(-1, 1)\n",
    "    \n",
    "    return descriptor.flatten()\n",
    "\n",
    "def compare_descriptors(desc1, desc2):\n",
    "    return euclidean_distances([desc1], [desc2])[0][0]\n",
    "\n",
    "def classify_images(reference1, reference2, folder, output_folder1, output_folder2):\n",
    "    grid_size = (128, 128)\n",
    "    weights = custom_weights(grid_size, center_fraction=0.4, center_weight=2.0)\n",
    "    \n",
    "    ref1_descriptor = color_layout_descriptor(reference1, grid_size, weights)\n",
    "    ref2_descriptor = color_layout_descriptor(reference2, grid_size, weights)\n",
    "    \n",
    "    os.makedirs(output_folder1, exist_ok=True)\n",
    "    os.makedirs(output_folder2, exist_ok=True)\n",
    "    \n",
    "    for filename in os.listdir(folder):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        \n",
    "        if os.path.isfile(filepath):\n",
    "            image = cv2.imread(filepath)\n",
    "            image_descriptor = color_layout_descriptor(image, grid_size, weights)\n",
    "            \n",
    "            similarity_to_ref1 = compare_descriptors(ref1_descriptor, image_descriptor)\n",
    "            similarity_to_ref2 = compare_descriptors(ref2_descriptor, image_descriptor)\n",
    "            \n",
    "            if similarity_to_ref1 < similarity_to_ref2:\n",
    "                shutil.copy(filepath, os.path.join(output_folder1, filename))\n",
    "            else:\n",
    "                shutil.copy(filepath, os.path.join(output_folder2, filename))\n",
    "\n",
    "reference_image1 = cv2.imread('detected_players/player_3.jpg')\n",
    "reference_image2 = cv2.imread('detected_players/player_4.jpg')\n",
    "\n",
    "input_folder = 'two_players_top'\n",
    "\n",
    "output_folder1 = 'player3_colour'\n",
    "output_folder2 = 'player4_colour'\n",
    "\n",
    "classify_images(reference_image1, reference_image2, input_folder, output_folder1, output_folder2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 4: Using a weighted combination of Clip and Colour Layout descriptor to classify the players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "def calculate_similarity(image_path, ref_features):\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    image_features = model.encode_image(image)\n",
    "    \n",
    "    ref_features_tensor = torch.tensor(ref_features).to(device)\n",
    "\n",
    "    similarity = cos(image_features[0], ref_features_tensor[0]).item()\n",
    "    return (similarity + 1) / 2\n",
    "\n",
    "def custom_weights(grid_size=(128, 128), center_fraction=0.4, center_weight=2.0):\n",
    "    \"\"\"Generate a custom weight matrix with higher weights in the center.\"\"\"\n",
    "    weights = np.ones(grid_size)\n",
    "    center_start = int((1 - center_fraction) * grid_size[0] / 2)\n",
    "    center_end = grid_size[0] - center_start\n",
    "\n",
    "    for i in range(center_start, center_end):\n",
    "        for j in range(center_start, center_end):\n",
    "            weights[i, j] = center_weight\n",
    "\n",
    "    return weights\n",
    "\n",
    "def color_layout_descriptor(image, grid_size=(128, 128), weights=None):\n",
    "    resized_image = cv2.resize(image, (256, 256))\n",
    "    \n",
    "    ycrcb_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2YCrCb)\n",
    "    \n",
    "    height, width, _ = ycrcb_image.shape\n",
    "    grid_h, grid_w = height // grid_size[0], width // grid_size[1]\n",
    "    \n",
    "    descriptor = []\n",
    "    for i in range(0, height, grid_h):\n",
    "        for j in range(0, width, grid_w):\n",
    "            cell = ycrcb_image[i:i+grid_h, j:j+grid_w]\n",
    "            mean_color = np.mean(cell.reshape(-1, 3), axis=0)\n",
    "            descriptor.append(mean_color)\n",
    "    \n",
    "    descriptor = np.array(descriptor)\n",
    "    \n",
    "    if weights is not None:\n",
    "        descriptor *= weights.reshape(-1, 1)\n",
    "    \n",
    "    return descriptor.flatten()\n",
    "\n",
    "def compare_descriptors(desc1, desc2):\n",
    "    return euclidean_distances([desc1], [desc2])[0][0]\n",
    "\n",
    "def classify_images(reference1, reference2, folder, output_folder1, output_folder2, weights=(0.5, 0.5)):\n",
    "    grid_size = (128, 128)\n",
    "    custom_weight_matrix = custom_weights(grid_size, center_fraction=0.4, center_weight=2.0)\n",
    "    \n",
    "    ref1_descriptor = color_layout_descriptor(reference1, grid_size, custom_weight_matrix)\n",
    "    ref2_descriptor = color_layout_descriptor(reference2, grid_size, custom_weight_matrix)\n",
    "    \n",
    "    os.makedirs(output_folder1, exist_ok=True)\n",
    "    os.makedirs(output_folder2, exist_ok=True)\n",
    "    \n",
    "    for filename in os.listdir(folder):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        \n",
    "        if os.path.isfile(filepath):\n",
    "            image = cv2.imread(filepath)\n",
    "            image_descriptor = color_layout_descriptor(image, grid_size, custom_weight_matrix)\n",
    "            \n",
    "            sim1 = calculate_similarity(filepath, ref1_descriptor)\n",
    "            sim2 = calculate_similarity(filepath, ref2_descriptor)\n",
    "            \n",
    "            similarity_to_ref1 = compare_descriptors(ref1_descriptor, image_descriptor)\n",
    "            similarity_to_ref2 = compare_descriptors(ref2_descriptor, image_descriptor)\n",
    "            \n",
    "            weighted_score1 = weights[0] * sim1 + weights[1] * (1 - similarity_to_ref1)\n",
    "            weighted_score2 = weights[0] * sim2 + weights[1] * (1 - similarity_to_ref2)\n",
    "            \n",
    "            if weighted_score1 > weighted_score2:\n",
    "                shutil.copy(filepath, os.path.join(output_folder1, filename))\n",
    "            else:\n",
    "                shutil.copy(filepath, os.path.join(output_folder2, filename))\n",
    "\n",
    "reference_image1 = cv2.imread('detected_players/player_1.jpg')\n",
    "reference_image2 = cv2.imread('detected_players/player_2.jpg')\n",
    "\n",
    "input_folder = 'two_players_bot'\n",
    "\n",
    "output_folder1 = 'player1_image_comb'\n",
    "output_folder2 = 'player2_image_comb'\n",
    "\n",
    "classify_images(reference_image1, reference_image2, input_folder, output_folder1, output_folder2, weights=(0.6, 0.4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
